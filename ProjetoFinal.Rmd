---
title: "Projeto Final de Processamento e Visualização de Dados"
author: "Grupo: Felipe, Lucas e Rafael"
date: "01/03/2022"
output: html_document
---

## Importando Dados & Bibliotecas

```{r Importando Dados e Bibliotecas}
library(VIM)
library(outliers)
library(EnvStats)
library(tidyverse)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)

data <- read_csv("data_cleaned_2021.csv")
set.seed(10)
```

## Dados Faltantes

A seguir, transformamos os valores faltantes em nosso *dataset*, representado pelos valores -1 e "na" para a constante NA, de modo a facilitar suas localizações no conjunto de dados e suas quantidades pelo uso de funções pré-definidas em R.

### Calculando Quantidade de Dados Faltantes

```{r Dados Faltantes}

# Transformando strings "na" e inteiros -1 em NA.
for(i in 1:ncol(data)){
  data[(data[,i]=="na" | data[,i]==-1),i] <- NA

}

# Funcao que verifica a quantidade de dados faltantes por coluna, retornando as
#  informacoes em um 'tibble'.
verifica_faltantes <- function(data) {
  num_faltantes <- c()    # Número de observações faltantes.
  prop_faltantes <- c()   # Proporção de dados faltantes.
  for (i in 1:ncol(data)) {
    num_nas =  sum(is.na(data[,i]))
    num_faltantes <- append(num_faltantes, num_nas)
    prop_faltantes <- append(prop_faltantes, num_nas * 100/nrow(data))
  }
  faltantes <- tibble(colunas = names(data), num_faltantes = num_faltantes, prop_faltantes = prop_faltantes)
  return(faltantes)
  
}

# Chama a function para análise.
faltantes <- verifica_faltantes(data)
faltantes
```

Logo, segundo o *tibble* gerado acima, as observações nesse conjunto representam os atributos, com esses indicando fatores como quantidade de dados faltantes, além de sua proporção comparado ao *dataset* total.

Com o objetivo de facilitar essa análise, a seguir é gerada uma imagem que busca contemplar visualmente as relaçõoes de atributos faltantes com o total de atributos do *dataset*, permitindo melhores abordagens de imputação.

```{r Plotando Dados Faltantes}
# Preparing data to plot using `barplot`
totalAndMissingValuesPerCol <- c()
for (i in 1:nrow(faltantes)) {
  # Faltantes
  totalAndMissingValuesPerCol <- append(totalAndMissingValuesPerCol,
                                        faltantes[i, 2])
  # Não Faltantes
  totalAndMissingValuesPerCol <- append(totalAndMissingValuesPerCol,
                                        (nrow(data) - faltantes[i, 2]))
}
totalAndMissingValuesPerCol <- matrix(totalAndMissingValuesPerCol,
                                      nrow = 2, ncol=ncol(data))
rownames(totalAndMissingValuesPerCol) <- c("TOTAL", "MISSING")
colnames(totalAndMissingValuesPerCol) <- colnames(data)

barplot(totalAndMissingValuesPerCol,
        main = "Valores faltantes no conjunto de dados",xlab = "Atributos",
        col = c("#0060E5","#14CE75"))

# legend for barplot
legend("topright",
       c("Valores totais","Valores faltantes"),
       fill = c("#0060E5","#14CE75"))
```

### Imputando valores faltantes para aqueles > 5%

A seguir, conforme descrito na especificação do trabalho, *para os atributos que possuam uma porcentagem maior que 5% de valores faltantes, deve ser proposta uma forma de tratá-los"*. Com isso, conforme explicitado no artigo em desenvolvimento, temos 3 tipos de atributos para aplicar 3 tipos de imputação.

```{r Imputação Constante}

# Imputacao de valores contínuos e inteiros
data$Rating[is.nan(data$Rating)] <- mean(data$Rating, na.rm = TRUE)
data$Founded[is.nan(data$Founded)] <- median(data$Founded, na.rm = TRUE)
data$Age[is.nan(data$Age)] <- as.integer(mean(data$Age, na.rm = TRUE))

# Remocao do atributo 'Competitors', 'seniority\_by\_title' e 'Degree' por serem superiores a 60%
data$Competitors <- NULL
data$seniority_by_title <- NULL
data$Degree <- NULL

# Imputacao dos valores do tipo 'string' a partir do uso do kNN
data <- kNN(data)

## Removendo colunas extras inseridas pelo kNN
data <- select(data, -c(39:78))

faltantes <- verifica_faltantes(data)
```

## Ruídos e Outliers

A secção a seguir destina-se a realizar os testes com a finalidade de descobrir a presença de outliers e dados ruidosos.

```{r Ruidos e Outliers}



outliers_com_grubbs <- function(column) {
  # Aplicando o Teste de Grubbs
  print(paste0("-----Aplicando o Teste de Grubbs-----"))
  grubbs <- grubbs.test(column, opposite = FALSE)
  print(grubbs)
  grubbs <- grubbs.test(column, opposite = TRUE)
  print(grubbs)
}

outliers_com_dixon <- function(column) {
  # selecionando uma amostra aleatoria sem substituicao
  sub_data <- sample(column, size = 30, replace = FALSE)
  
  # ordenando amostra
  sub_data <- sort(sub_data)
  
  # Aplicando o Teste de Dixon
  print(paste0("-----Aplicando o Teste de Dixon-----"))
  dixon <- dixon.test(sub_data, opposite = FALSE)
  print(dixon)
  dixon <- dixon.test(sub_data, opposite = TRUE)
  print(dixon)
}

outliers_com_rosner <- function(column) {
  # Aplicando o Teste de Rosner
  print(paste0("-----Aplicando o Teste de Rosner-----"))
  rosner <- rosnerTest(column, k = 6)
  print(rosner$all.stats)
}

outliers_com_Hampel <- function(column, colName) {
  cat("\n")
  # Aplicando o metodo Filtro de Hampel
  print(paste0("-----Aplicando o Filtro de Hampel-----"))
  limite_minimo = median(column) - 3 * mad(column)
  limite_maximo = median(column) + 3 * mad(column)
  
  hampel <- data %>% filter(column < limite_minimo | column > limite_maximo) %>% select(index, colName)
  print(hampel)
}

# Verificando quais atributos sao binarios
eh_binario <- apply(data, 2, function(x) {all(x %in% 0:1)})
nomes_colunas <- colnames(data)

# Execução dos testes
for(i in 2:ncol(data)){
  if ((typeof(data[1,i]) != "character") & !eh_binario[nomes_colunas[i]]) {
    print(paste0("************************************"))
    print(paste0("Testando outliers em ", colnames(data[i]), "."))
    print(paste0("************************************"))
    outliers_com_grubbs(data[,i])
    outliers_com_dixon(data[,i])
    outliers_com_rosner(data[,i])
    outliers_com_Hampel(data[,i], nomes_colunas[i])
    cat("\n\n")
  }
}

```

### Redução de dados

Para a redução dos dados, será utilizado o algoritmo PCA sobre o subconjunto de dados referente as tecnologias das vagas.

Pela análise dos resultados obtidos do PCA, em especial pela variância, é possível observar que esse subconjunto de dados possa ser razoavelmente bem representado a partir de 8 componentes principais gerados pelo PCA, permitindo uma representatividade de ~74%.A plotagem desses dados seria possível em um plano de 8 dimensões, algo visivelmente difícil de se compreender. Em contrapartida, o uso de 3 componentes permite um plano tridimensional, mas é capaz de representar apenas ~43% do total de dados.

Dessa forma, é evidente que a utilização do PCA para visualização de dados nesse caso não é muito

```{r Reducao de dados}
requirements <- select(data, Python, spark, aws, excel, sql, sas, keras, pytorch, scikit, tensor, hadoop, tableau, bi, flink, mongo, google_an)

requirements_pca <- prcomp(requirements, center = TRUE, scale. = TRUE)

as.data.frame(requirements_pca$x)

summary(requirements_pca)
```

```{r Imagem de 3 componentes}
ggbiplot(requirements_pca)
```