---
title: "Projeto Final de Processamento e Visualização de Dados"
author: "Grupo: Felipe, Lucas e Rafael"
date: "01/03/2022"
output: html_document
---

## Importando Dados & Bibliotecas

```{r Importando Dados e Bibliotecas}
library(VIM)
library(mice)
library(outliers)
library(EnvStats)
library(tidyverse)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)

data <- read_csv("data_cleaned_2021.csv")
set.seed(10)
```
## Transformando dados

```{r Atributos irrelevantes}
# Transformação do atributo 'Size'
#  - Ordenação manual e remoção do valor 'unknown'
levels_size <- levels(factor(data$Size))
data$Size <- as.numeric(factor(data$Size, levels = c(levels_size[1],
                                          levels_size[7],
                                          levels_size[4],
                                          levels_size[6],
                                          levels_size[3],
                                          levels_size[5],
                                          levels_size[2])))

# Transformação do atributo 'Revenue'
#  - Ordenação manual e remoção do valor 'unknown'
levels_Revenue <- levels(factor(data$Revenue))
data$Revenue <- as.numeric(factor(data$Revenue, levels = c(levels_Revenue[12],
                                                levels_Revenue[2],
                                                levels_Revenue[9],
                                                levels_Revenue[3],
                                                levels_Revenue[7], 
                                                levels_Revenue[10],
                                                levels_Revenue[5],
                                                levels_Revenue[11],
                                                levels_Revenue[1],
                                                levels_Revenue[6],
                                                levels_Revenue[8],
                                                levels_Revenue[4])))
```

### Quantidade de Dados Faltantes

```{r Dados Faltantes}

# Transformando strings "na" e inteiros -1 em NA.
for(i in 1:ncol(data)){
  data[((data[,i]=="na" | data[,i]==-1) & !is.na(data[,i])), i] <- NA
}

# Funcao que verifica a quantidade de dados faltantes por coluna, retornando as
#  informacoes em um 'tibble'.
verifica_faltantes <- function(data) {
  num_faltantes <- c()    # Número de observações faltantes.
  prop_faltantes <- c()   # Proporção de dados faltantes.
  for (i in 1:ncol(data)) {
    num_nas =  sum(is.na(data[,i]))
    num_faltantes <- append(num_faltantes, num_nas)
    prop_faltantes <- append(prop_faltantes, num_nas * 100/nrow(data))
  }
  faltantes <- tibble(colunas = names(data), num_faltantes = num_faltantes,
                      prop_faltantes = prop_faltantes)
  return(faltantes)
}

# Chama a function para análise.
faltantes <- verifica_faltantes(data)
faltantes
```


```{r Plotando Dados Faltantes}
# Preparando dados para realizar a plotagem.
totalAndMissingValuesPerCol <- c()
for (i in 1:nrow(faltantes)) {
  # Faltantes
  totalAndMissingValuesPerCol <- append(totalAndMissingValuesPerCol,
                                        faltantes[i, 2])
  # Não Faltantes
  totalAndMissingValuesPerCol <- append(totalAndMissingValuesPerCol,
                                        (nrow(data) - faltantes[i, 2]))
}
totalAndMissingValuesPerCol <- matrix(totalAndMissingValuesPerCol,
                                      nrow = 2, ncol=ncol(data))
rownames(totalAndMissingValuesPerCol) <- c("TOTAL", "MISSING")
colnames(totalAndMissingValuesPerCol) <- colnames(data)

# Plot. O 'barplot' permite que mais de 2 colunas sejam adicionada impressas em
#  apenas uma.
barplot(totalAndMissingValuesPerCol,
        main = "Valores faltantes no conjunto de dados",xlab = "Atributos",
        col = c("#0060E5","#14CE75"))

# legend for barplot
legend("topright",
       c("Valores totais","Valores faltantes"),
       fill = c("#0060E5","#14CE75"))
```

```{r Imputação Constante}

# Imputacao de valores contínuos e inteiros
data$Rating[is.nan(data$Rating)] <- mean(data$Rating, na.rm = TRUE)
data$Founded[is.nan(data$Founded)] <- median(data$Founded, na.rm = TRUE)
data$Age[is.nan(data$Age)] <- as.integer(mean(data$Age, na.rm = TRUE))

# Remocao do atributo 'Competitors', 'seniority\_by\_title' e 'Degree' por serem
#  superiores a 60%
data$Competitors <- NULL
data$seniority_by_title <- NULL
data$Degree <- NULL

# Imputacao dos valores do tipo 'string' a partir do uso do kNN
data <- kNN(data)

## Removendo colunas extras inseridas pelo kNN
data <- select(data, -c(39:78))

faltantes <- verifica_faltantes(data)
```

## Verificação de possíveis atributos redundantes

```{r Verificacao de redundancia}
calcular_covariancia <- function(vec1, vec2) {
  covariancia <- 0
  for (i in 1:length(vec1)) {
    dif_a = vec1[i] - mean(vec1, na.rm=TRUE) 
    dif_b = vec2[i] - mean(vec2, na.rm=TRUE)
    covariancia <- covariancia + dif_a*dif_b
  }
  covariancia <- covariancia/(length(vec1))
}

calcular_corrs_numericos <- function(dataset_numeric, na.rm=TRUE){
  redundantes <- c()
  for(i in 1:ncol(dataset_numeric)){
    for(j in 1:ncol(dataset_numeric)){
      if( i==j || i<j ){
        next
      }
      vec1 <- as.vector(dataset_numeric[,i])
      vec2 <- as.vector(dataset_numeric[,j])
      r <- calcular_covariancia(vec1,vec2)/(sd(vec1)*sd(vec2))
      soma_quadrados <- sum((vec1 - mean(vec2)) ^ 2)
      soma_quadrados_residuais <- sum((vec1 - vec2)^2)
      r_2 <- 1 - (soma_quadrados_residuais/soma_quadrados)
      print(paste0("R: ", r, " R2:", r_2))
      if(((r > 0.7) || (r < -0.7)) || (r_2 > 0.7)) {
        print(paste(names(data_numeric[i]),
                    " e ", 
                    names(data_numeric[j]),
                    "são correlacionados"))
        append(redundantes, names(data_numeric[j]))
        redundantes <- union(redundantes, names(data_numeric[j]))
        print(paste(r,", ",r_2))
      }
    }
    
  }
  return(redundantes)

}

calcula_corrs_nominais <- function(dataset_nominal, na.rm=TRUE){
  redundantes <- c()
  for(i in 1:ncol(dataset_nominal)){
    for(j in 1:ncol(dataset_nominal)){
      if(i==j||i<j){
        next
      }
      corr = chisq.test(data_nominal[,i], data_nominal[,j],
                        simulate.p.value = TRUE)
      if(corr$p.value <= 0.05){
        print(paste(names(data_nominal[i]),
                    " e ", 
                    names(data_nominal[j]),
                    "são correlacionados"))
        append(redundantes, names(data_nominal[j]))
        redundantes <- union(redundantes, names(data_nominal[j]))
      }
    }
    
  }
  return(redundantes)
}



# Invoca as funções definidas acima.
data_numeric <- data %>% select_if(is.numeric)
redundantes <- calcular_corrs_numericos(data_numeric)
print(paste("São redundantes e podem ser removidos: ", toString(redundantes)))
```


## Amostragem e Técnicas de Imputação

```{r Amostragem Simples}
## Quantidade de amostras a serem selecionadas (30%)
qnt_amostra <- nrow(data) * 0.3

## Amostragem aleatória simples sem substituição
data_amostragem <- sample_n(data, qnt_amostra, replace = FALSE)

## Adicionando valores faltantes.
## ~ 4 valores
positions <- sample(x = c(0:nrow(data_amostragem)), 
                    size = 0.02 * nrow(data_amostragem))
data_amostragem$`Avg Salary(K) Amostrado` <- data_amostragem$`Avg Salary(K)`
```

```{r Imputacao}
## Avaliacao das imputacoes por R^2
r_squared <- function(obs, pred) {
  rss <- sum((pred - obs) ^ 2)
  tss <- sum((obs - mean(obs)) ^ 2)
  rs <- 1 - rss/tss
  rs
}

mode <- function(column) {
   value <- unique(column)
   value[which.max(tabulate(match(column, value)))]
}

print(paste("R Quadrado: ",
            r_squared(obs = data_amostragem$`Avg Salary(K)`,
                      pred = data_amostragem$`Avg Salary(K)`)))


## Subtituição por valor Médio
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- NA
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- mean(data_amostragem$`Avg Salary(K) Amostrado`, na.rm = TRUE)

print(paste("R Quadrado Para Imputação Constante (Média): ",
            r_squared(obs = data_amostragem$`Avg Salary(K)`,
                      pred = data_amostragem$`Avg Salary(K) Amostrado`)))

## Subtituição por valor Mediano
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- NA
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- median(data_amostragem$`Avg Salary(K) Amostrado`, na.rm = TRUE)

print(paste("R Quadrado Para Imputação Constante (Mediana): ",
            r_squared(obs = data_amostragem$`Avg Salary(K)`,
                      pred = data_amostragem$`Avg Salary(K) Amostrado`)))

## Subtituição por valor Moda
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- NA
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- mode(data_amostragem$`Avg Salary(K) Amostrado`)

print(paste("R Quadrado Para Imputação Constante (Moda): ",
            r_squared(obs = data_amostragem$`Avg Salary(K)`,
                      pred = data_amostragem$`Avg Salary(K) Amostrado`)))

## KNN
data_amostragem$`Avg Salary(K) Amostrado`[positions] <- NA
data_amostragem <- kNN(data_amostragem)
data_amostragem <- select(data_amostragem, -c(40:78))

print(paste("R Quadrado Para KNN: ",
            r_squared(obs = data_amostragem$`Avg Salary(K)`,
                      pred = data_amostragem$`Avg Salary(K) Amostrado`)))

## Regressão Linear Bayesiana
colnames(data_amostragem) <- make.names(colnames(data_amostragem))
data_amostragem$`Avg.Salary.K..Amostrado`[positions] <- NA

data_amostragem_mice <- mice(
  data_amostragem,
  method = 'norm',
  m = 1,
  remove_collinear = FALSE)

data_amostragem <- as.data.frame(complete(data_amostragem_mice))

print(paste("R Quadrado Para Regressão Linear Bayesiana: ",
            r_squared(obs = data_amostragem$`Avg.Salary.K.`,
                      pred = data_amostragem$`Avg.Salary.K..Amostrado`)))
```

## Ruídos e Outliers

```{r Ruidos e Outliers}

outliers_com_grubbs <- function(column) {
  # Aplicando o Teste de Grubbs
  print(paste("-----Aplicando o Teste de Grubbs-----"))
  grubbs <- grubbs.test(column, opposite = FALSE)
  print(grubbs)
  grubbs <- grubbs.test(column, opposite = TRUE)
  print(grubbs)
}

outliers_com_dixon <- function(column) {
  # selecionando uma amostra aleatoria sem substituicao
  sub_data <- sample(column, size = 30, replace = FALSE)
  
  # ordenando amostra
  sub_data <- sort(sub_data)
  
  # Aplicando o Teste de Dixon
  print(paste("-----Aplicando o Teste de Dixon-----"))
  dixon <- dixon.test(sub_data, opposite = FALSE)
  print(dixon)
  dixon <- dixon.test(sub_data, opposite = TRUE)
  print(dixon)
}

outliers_com_rosner <- function(column) {
  # Aplicando o Teste de Rosner
  print(paste("-----Aplicando o Teste de Rosner-----"))
  rosner <- rosnerTest(column, k = 6)
  print(rosner$all.stats)
}

outliers_com_Hampel <- function(column, colName) {
  cat("\n")
  # Aplicando o metodo Filtro de Hampel
  print(paste("-----Aplicando o Filtro de Hampel-----"))
  limite_minimo = median(column) - 3 * mad(column)
  limite_maximo = median(column) + 3 * mad(column)
  
  hampel <- data %>% filter(column < limite_minimo | column > limite_maximo) %>% select(index, colName)
  print(hampel)
}

# Verificando quais atributos sao binarios
eh_binario <- apply(data, 2, function(x) {all(x %in% 0:1)})
nomes_colunas <- colnames(data)

# Execução dos testes
for(i in 2:ncol(data)){
  if ((typeof(data[1,i]) != "character") & !eh_binario[nomes_colunas[i]]) {
    print(paste("************************************"))
    print(paste("Testando outliers em ", colnames(data[i]), "."))
    print(paste("************************************"))
    outliers_com_grubbs(data[,i])
    outliers_com_dixon(data[,i])
    outliers_com_rosner(data[,i])
    outliers_com_Hampel(data[,i], nomes_colunas[i])
    cat("\n\n")
  }
}

```

## Redução de atributos

```{r Reducao de atributos numéricos}
requirements <- select(data, Rating, Size, Founded, Revenue, Hourly, `Employer provided`,
                       `Lower Salary`, `Upper Salary`, `Avg Salary(K)`, Age)

requirements_pca <- prcomp(requirements, center = TRUE, scale. = TRUE)

as.data.frame(requirements_pca$x)

summary(requirements_pca)
```

```{r Imagem de 3 componentes}
ggbiplot(requirements_pca)
```

```{r Reducao de atributos booleanos}
requirements <- select(data, Python, spark, aws, excel, sql, sas, keras, pytorch, scikit, tensor, hadoop, tableau, bi, flink, mongo, google_an)

requirements_pca <- prcomp(requirements, center = TRUE, scale. = TRUE)

as.data.frame(requirements_pca$x)

summary(requirements_pca)
```

```{r Imagem de 3 componentes}
ggbiplot(requirements_pca)
```

## Visualização de dados

### Visualização das **hard-skills** mais requisitadas

```{r Visualizacao1}
# Selecionando apenas as colunas necessárias para geração do gráfico.
skills <- select(data, Python, spark, aws, excel, sql, sas, keras, pytorch, scikit, tensor, hadoop, tableau, bi, flink, mongo, google_an)

# Criação de um novo dataframe
sum_skills = data.frame()
# Para cada ferramenta, contabiliza o total de ocorrências no conjunto.
for (i in 1:ncol(skills)) {
  sum_skills[i,1] = sum(skills[,i])
}
# Juntando ao data.frame os nomes das ferramentas e atribui nomes as colunas.
sum_skills[,2] = colnames(skills)
colnames(sum_skills) = c('Occur', 'Skills')

# Por fim, realiza a plotagem do gráfico.
grafico_skills <- ggplot(data = sum_skills) + 
  geom_bar(stat = "identity", position = position_dodge(), mapping = aes(x = as.factor(Skills), as.factor(Occur)))
grafico_skills
```